{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d356d88",
   "metadata": {},
   "source": [
    "# CORD-19 Dataset Analysis and Visualization\n",
    "\n",
    "This notebook implements a comprehensive analysis of the CORD-19 (COVID-19 Open Research Dataset) with focus on the metadata of research papers. The analysis includes data exploration, cleaning, visualization, and creation of an interactive Streamlit application.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup Development Environment\n",
    "2. Download and Load Dataset\n",
    "3. Basic Data Exploration\n",
    "4. Data Cleaning and Preparation\n",
    "5. Exploratory Data Analysis\n",
    "6. Create Visualizations\n",
    "7. Build Streamlit Application\n",
    "8. Package and Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39acfb9",
   "metadata": {},
   "source": [
    "# 1. Setup Development Environment\n",
    "\n",
    "First, we'll set up our Colab environment and install required packages. We'll need:\n",
    "- pandas: for data manipulation\n",
    "- matplotlib and seaborn: for visualization\n",
    "- nltk: for text analysis\n",
    "- wordcloud: for generating word clouds\n",
    "- streamlit: for the interactive web application\n",
    "- kaggle: for downloading the dataset\n",
    "\n",
    "Note: In Colab, some of these packages may already be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas matplotlib seaborn nltk wordcloud streamlit kaggle --quiet\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4c8bc",
   "metadata": {},
   "source": [
    "# 2. Download and Load Dataset\n",
    "\n",
    "We'll download the CORD-19 dataset metadata file from Kaggle. To use the Kaggle API, make sure you have:\n",
    "1. A Kaggle account\n",
    "2. API credentials (kaggle.json) file\n",
    "\n",
    "The dataset we're using is the CORD-19 research challenge dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5eeaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# Note: For quick testing, we'll use a small sample. For the full dataset, uncomment the kaggle command\n",
    "import os\n",
    "\n",
    "# Try to use existing sample first\n",
    "if os.path.exists('metadata_sample.csv'):\n",
    "    print(\"Using existing metadata_sample.csv\")\n",
    "else:\n",
    "    print(\"Downloading sample metadata...\")\n",
    "    !wget -q -O metadata_sample.csv https://raw.githubusercontent.com/luanwachira-bit/week8-py-codes/main/metadata_sample.csv\n",
    "\n",
    "# To download full dataset from Kaggle (uncomment these lines):\n",
    "# !kaggle datasets download -d allen-institute-for-ai/CORD-19-research-challenge --file metadata.csv\n",
    "# !unzip -o metadata.csv.zip metadata.csv\n",
    "\n",
    "# Function to load and validate the dataset\n",
    "def load_dataset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the CORD-19 metadata CSV file into a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the metadata CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset (try full dataset first, fall back to sample)\n",
    "if os.path.exists('metadata.csv'):\n",
    "    df = load_dataset('metadata.csv')\n",
    "else:\n",
    "    print(\"Full metadata.csv not found, using sample...\")\n",
    "    df = load_dataset('metadata_sample.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Kaggle credentials\n",
    "# This cell shows how to do it safely - you'll need to provide your own credentials\n",
    "import os\n",
    "import json\n",
    "\n",
    "def setup_kaggle():\n",
    "    \"\"\"Configure Kaggle credentials safely.\"\"\"\n",
    "    kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "    if not os.path.exists(kaggle_dir):\n",
    "        os.makedirs(kaggle_dir)\n",
    "    \n",
    "    kaggle_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "    if not os.path.exists(kaggle_path):\n",
    "        print(\"Please enter your Kaggle credentials:\")\n",
    "        username = input(\"Username: \")\n",
    "        key = input(\"API Key: \")\n",
    "        \n",
    "        credentials = {\n",
    "            \"username\": username,\n",
    "            \"key\": key\n",
    "        }\n",
    "        \n",
    "        with open(kaggle_path, 'w') as f:\n",
    "            json.dump(credentials, f)\n",
    "        os.chmod(kaggle_path, 0o600)\n",
    "        print(\"Credentials saved successfully!\")\n",
    "    else:\n",
    "        print(\"Kaggle credentials file already exists.\")\n",
    "\n",
    "# Run setup\n",
    "setup_kaggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure Kaggle and Download Dataset\n",
    "\n",
    "To download the CORD-19 dataset, we need to:\n",
    "1. Set up Kaggle API credentials (safely)\n",
    "2. Download the metadata.csv file\n",
    "\n",
    "**Important: Never share your Kaggle API key in notebook cells or public repositories!**\n",
    "\n",
    "Follow these steps to set up Kaggle:\n",
    "1. Go to kaggle.com → Account → 'Create New API Token'\n",
    "2. Download kaggle.json\n",
    "3. Run the cell below to configure the credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d6ecb",
   "metadata": {},
   "source": [
    "# 3. Basic Data Exploration\n",
    "\n",
    "Let's explore the basic characteristics of our dataset:\n",
    "1. Check data types of each column\n",
    "2. Identify missing values\n",
    "3. Generate basic statistics\n",
    "4. Understand the structure of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze dataset structure\n",
    "def analyze_dataset_structure(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and display the basic structure of the dataset\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset\n",
    "    \"\"\"\n",
    "    print(\"Data Types of Columns:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentages = (missing_values / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Values': missing_values,\n",
    "        'Percentage': missing_percentages\n",
    "    })\n",
    "    print(missing_info[missing_info['Missing Values'] > 0])\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "\n",
    "# Analyze dataset structure\n",
    "analyze_dataset_structure(df)\n",
    "\n",
    "# Display unique values in categorical columns\n",
    "print(\"\\nUnique values in selected columns:\")\n",
    "categorical_columns = ['source_x', 'journal']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f84acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the metadata.csv file using wget\n",
    "!wget -O metadata.csv https://github.com/allenai/cord19/raw/master/sample-metadata.csv\n",
    "\n",
    "# Function to load and validate the dataset\n",
    "def load_dataset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the CORD-19 metadata CSV file into a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the metadata CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "df = load_dataset('metadata.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ec789",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning and Preparation\n",
    "\n",
    "Now we'll clean the dataset and prepare it for analysis:\n",
    "1. Handle missing values\n",
    "2. Convert date columns to proper datetime format\n",
    "3. Create derived features\n",
    "4. Clean text data in titles and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning and feature engineering\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def clean_and_prepare(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform basic cleaning and add useful derived columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with CORD-19 metadata\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame with additional features\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    print(\"Standardized column names\")\n",
    "    \n",
    "    # Convert publish_time to datetime and extract year\n",
    "    if 'publish_time' in df.columns:\n",
    "        df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')\n",
    "        df['year'] = df['publish_time'].dt.year\n",
    "        print(f\"Converted dates, found years from {df['year'].min()} to {df['year'].max()}\")\n",
    "    else:\n",
    "        df['year'] = pd.NA\n",
    "        print(\"No publish_time column found\")\n",
    "    \n",
    "    # Process text columns with progress bar\n",
    "    text_columns = ['abstract', 'title']\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\nProcessing {col}...\")\n",
    "            df[col] = df[col].fillna('').astype(str)\n",
    "            df[f'{col}_word_count'] = df[col].str.split().apply(len)\n",
    "            mean_words = df[f'{col}_word_count'].mean()\n",
    "            print(f\"Average {col} length: {mean_words:.1f} words\")\n",
    "    \n",
    "    # Handle source information\n",
    "    if 'source_x' in df.columns:\n",
    "        df['source_x'] = df['source_x'].fillna('unknown')\n",
    "        sources = df['source_x'].value_counts()\n",
    "        print(f\"\\nFound {len(sources)} different sources\")\n",
    "        print(\"Top 3 sources:\", sources.head(3).to_dict())\n",
    "    \n",
    "    # Drop completely empty columns\n",
    "    empty_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "    if empty_cols:\n",
    "        df = df.drop(columns=empty_cols)\n",
    "        print(f\"\\nDropped {len(empty_cols)} empty columns: {empty_cols}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nFinal dataset shape:\", df.shape)\n",
    "    print(\"Columns:\", sorted(df.columns))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply cleaning with timing\n",
    "import time\n",
    "start = time.time()\n",
    "df = clean_and_prepare(df)\n",
    "print(f\"\\nCleaning completed in {time.time() - start:.1f} seconds\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302dba3",
   "metadata": {},
   "source": [
    "# 5. Exploratory Data Analysis\n",
    "\n",
    "We'll perform basic analyses: publications by year, top journals, title word frequencies, and source distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9601bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications by year\n",
    "year_counts = df['year'].value_counts(dropna=True).sort_index()\n",
    "print('Publications by year:')\n",
    "display(year_counts)\n",
    "\n",
    "# Top journals\n",
    "if 'journal' in df.columns:\n",
    "    top_journals = df['journal'].fillna('unknown').value_counts().head(15)\n",
    "    print('\n",
    "Top journals:')\n",
    "    display(top_journals)\n",
    "else:\n",
    "    top_journals = pd.Series(dtype=int)\n",
    "\n",
    "# Source distribution\n",
    "if 'source_x' in df.columns:\n",
    "    source_counts = df['source_x'].value_counts()\n",
    "    print('\n",
    "Source distribution:')\n",
    "    display(source_counts)\n",
    "else:\n",
    "    source_counts = pd.Series(dtype=int)\n",
    "\n",
    "# Simple title word frequency\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    # Lowercase, remove non-alpha chars, and split\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = [t for t in text.split() if len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "title_tokens = Counter()\n",
    "for t in df['title'].fillna(''):\n",
    "    title_tokens.update(tokenize_text(t))\n",
    "\n",
    "top_title_words = pd.Series(dict(title_tokens.most_common(50)))\n",
    "print('\n",
    "Top title words:')\n",
    "display(top_title_words.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bfe21",
   "metadata": {},
   "source": [
    "# 6. Visualizations\n",
    "\n",
    "Create plots for publications over time, top journals, title word cloud, and source distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7532ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications over time plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "year_counts.plot(kind='bar')\n",
    "plt.title('Publications by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of publications')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top journals bar chart\n",
    "if not top_journals.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_journals.sort_values().plot(kind='barh', color='C1')\n",
    "    plt.title('Top Journals (by number of papers)')\n",
    "    plt.xlabel('Number of papers')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Word cloud for titles\n",
    "if not top_title_words.empty:\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(title_tokens)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Title Word Cloud')\n",
    "    plt.show()\n",
    "\n",
    "# Source distribution pie chart\n",
    "if not source_counts.empty:\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    source_counts.head(10).plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.ylabel('')\n",
    "    plt.title('Top Sources')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbbf00",
   "metadata": {},
   "source": [
    "# 7. Streamlit Application\n",
    "\n",
    "We'll provide a lightweight Streamlit app (`streamlit_app.py`) that loads the cleaned metadata and shows interactive controls for year range, top journals and a data table sample. In Colab, Streamlit can be run using `ngrok` or `localtunnel` (not included here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11157b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and configure ngrok\n",
    "!pip install pyngrok --quiet\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Create streamlit_app.py if it doesn't exist\n",
    "%%writefile streamlit_app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.title('CORD-19 Metadata Explorer')\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    try:\n",
    "        df = pd.read_csv('metadata.csv')\n",
    "    except:\n",
    "        df = pd.read_csv('metadata_sample.csv')\n",
    "        st.warning('Using sample dataset. Full metadata.csv not found.')\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Show basic stats\n",
    "st.write(f\"Dataset contains {len(df):,} papers\")\n",
    "\n",
    "# Year range filter\n",
    "years = pd.to_datetime(df['publish_time']).dt.year\n",
    "year_range = st.slider(\n",
    "    'Select year range',\n",
    "    int(years.min()),\n",
    "    int(years.max()),\n",
    "    (int(years.min()), int(years.max()))\n",
    ")\n",
    "\n",
    "# Filter data\n",
    "mask = (years >= year_range[0]) & (years <= year_range[1])\n",
    "filtered = df[mask]\n",
    "\n",
    "# Show visualizations\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.write('Papers by Year')\n",
    "    year_counts = years[mask].value_counts().sort_index()\n",
    "    st.bar_chart(year_counts)\n",
    "\n",
    "with col2:\n",
    "    st.write('Top Sources')\n",
    "    if 'source_x' in df.columns:\n",
    "        source_counts = filtered['source_x'].value_counts().head(10)\n",
    "        st.bar_chart(source_counts)\n",
    "\n",
    "# Show sample of papers\n",
    "st.write('Sample of Papers')\n",
    "st.dataframe(filtered[['title', 'publish_time', 'journal']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Streamlit in Colab\n",
    "\n",
    "To run the Streamlit app in Colab, we'll:\n",
    "1. Install pyngrok for tunneling\n",
    "2. Start Streamlit with a public URL\n",
    "3. Access the app through the provided link\n",
    "\n",
    "Note: The ngrok tunnel closes when the notebook disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b125547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Streamlit with ngrok tunnel\n",
    "# Note: This cell will display the public URL when run\n",
    "def run_streamlit():\n",
    "    from pyngrok import ngrok\n",
    "    \n",
    "    # Start ngrok tunnel\n",
    "    ngrok.kill()  # Kill any existing tunnels\n",
    "    streamlit_port = 8501\n",
    "    public_url = ngrok.connect(streamlit_port).public_url\n",
    "    print(f' * ngrok tunnel \"{public_url}\" -> \"http://127.0.0.1:{streamlit_port}\"')\n",
    "    \n",
    "    # Run Streamlit\n",
    "    !streamlit run streamlit_app.py --server.port {streamlit_port} &>/dev/null &\n",
    "    print('\\nStreamlit app is ready! Access it at:', public_url)\n",
    "\n",
    "# Run the app\n",
    "run_streamlit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a388e9e",
   "metadata": {},
   "source": [
    "# 8. Notes, Reflection and How to Run\n",
    "\n",
    "## How to run locally\n",
    "\n",
    "1. Install dependencies: `pip install -r requirements.txt`\n",
    "2. Ensure `metadata.csv` is in the repo root or use the bundled `metadata_sample.csv`\n",
    "3. Run Streamlit: `streamlit run streamlit_app.py`\n",
    "\n",
    "## Running in Colab\n",
    "\n",
    "Open this notebook in Colab, install packages, and download `metadata.csv`. For Streamlit in Colab use a tunnelling service (ngrok) — many guides exist online.\n",
    "\n",
    "## Reflection\n",
    "\n",
    "- We cleaned the dataset by converting dates, extracting years, and computing word counts for abstracts and titles.\n",
    "- Visualizations include publications by year, top journals, a title word cloud, and source distribution.\n",
    "- Challenges: the full CORD-19 dataset is large; use sampling or cloud resources for heavy analysis.\n",
    "\n",
    "If you'd like, I can now:\n",
    "\n",
    "- Add more in-depth text analysis (TF-IDF, topic modeling),\n",
    "- Improve Streamlit UI with more filters and caching, or\n",
    "- Create unit tests for the data cleaning functions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
